Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mirsky2016,
author = {Mirsky, Steve},
doi = {10.1038/scientificamerican0616-78},
issn = {0036-8733},
journal = {Scientific American},
month = {may},
number = {6},
pages = {78--78},
title = {{Arms Race}},
url = {http://www.nature.com/doifinder/10.1038/scientificamerican0616-78},
volume = {314},
year = {2016}
}
@misc{PECOTA,
author = {Silver, Nate},
booktitle = {Baseball Prospectus},
title = {{Baseball Prospectus Basics: The Science of Forecasting}},
url = {http://www.baseballprospectus.com/article.php?articleid=2659},
urldate = {2017-04-10},
year = {2004}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
isbn = {9781634393973},
issn = {10495258},
journal = {International Conference on Machine Learning - ICML 2014},
month = {may},
pages = {1188--1196},
pmid = {9377276},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Ngiam2011,
abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned ifmultiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evalu- ate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our mod- els are validated on the CUAVE and AVLet- ters datasets on audio-visual speech classifi- cation, demonstrating best published visual speech classification on AVLetters and effec- tive shared representation learning.},
archivePrefix = {arXiv},
arxivId = {1502.07209},
author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
doi = {10.1145/2647868.2654931},
eprint = {1502.07209},
isbn = {9781450306195},
issn = {9781450306195},
journal = {Proceedings of The 28th International Conference on Machine Learning (ICML)},
pages = {689--696},
pmid = {2609986},
title = {{Multimodal Deep Learning}},
url = {https://people.csail.mit.edu/khosla/papers/icml2011{\_}ngiam.pdf},
year = {2011}
}
@misc{Buchanan2015,
author = {Buchanan, Ben},
booktitle = {Over the Monster},
title = {{Do the Red Sox have anything in Jean Machi?}},
url = {http://www.overthemonster.com/2015/7/29/9066447/what-is-a-jean-machi-red-sox-giants},
urldate = {2017-04-24},
year = {2015}
}
@article{Kuehn2016,
abstract = {This paper develops a player evaluation framework that stresses the importance of accounting for complementarities between teammates when evaluating players. This is done by developing a probabilistic model of a basketball possession as a progression of events, where the probability of each event's occurrence is determined by the offensive players' skills, the defensive players' skills, and the complementarities between the skills of teammates. Evaluating players using this frame-work allows me to assess the substitutability between different game actions, the lineup-speciiic value a player brings to a team, and the players that are the best and worst teammates. It also al-lows me to separately identify the individual effect from the effect teammates have on a player's statistical production, and to evaluate whether player complementarities are valued in the market for NBA players in terms of higher salaries. I Iind that complementarities are under-valued, and that players are instead paid mainly for their individual statistical production.},
author = {Kuehn, Joseph},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Accounting for Complementary Skill Sets When Evaluating NBA Players' Values to a Speciiic Team}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2016/02/1425-Basketball.pdf},
year = {2016}
}
@misc{Sullivan2015,
author = {Sullivan, Jeff},
booktitle = {FanGraphs},
title = {{Dee Gordon Has Been Going Full Ichiro}},
url = {http://www.fangraphs.com/blogs/dee-gordon-has-been-going-full-ichiro/},
urldate = {2017-04-19},
year = {2015}
}
@misc{Spector2016,
author = {Spector, Jesse},
booktitle = {Sporting News},
title = {{Mike Trout vs. Bryce Harper: The SN50 rivalry that defines this generation of stars}},
url = {http://www.sportingnews.com/mlb/news/sn50-2016-best-baseball-players-mike-trout-bryce-harper/mk3kmorbiyhr1f7onb7t5pehq},
urldate = {2017-04-19},
year = {2016}
}
@misc{Alcorn2016,
author = {Alcorn, Michael A.},
title = {{Learning to Coach Football}},
url = {https://sites.google.com/view/michaelaalcorn/blog/learning-to-coach-football},
urldate = {2017-04-19},
year = {2016}
}
@misc{Judge2015,
author = {Judge, Jonathan},
booktitle = {Baseball Prospectu},
title = {{DRA: An In-Depth Discussion}},
url = {https://www.baseballprospectus.com/news/article/26196/prospectus-feature-dra-an-in-depth-discussion/},
urldate = {2017-10-27},
year = {2015}
}
@misc{Turkenkopf2015,
author = {Turkenkopf, Dan and Pavlidis, Harry and Judge, Jonathan},
booktitle = {Baseball Prospectus},
title = {{Introducing Deserved Run Average (DRA) And All Its Friends}},
url = {https://www.baseballprospectus.com/news/article/26195/prospectus-feature-introducing-deserved-run-average-draand-all-its-friends/},
urldate = {2017-10-27},
year = {2015}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@article{Bekkers2017,
abstract = {In soccer, both individual and team performance is crucial to win matches. Passing is the backbone of the game and forms the basis of important decisions made by managers and owners; such as buying players, picking offensive or defensive strategies or even defining a style of play. These decisions can be supported by analyzing how a player performs and how his style affects team performance. The flow of a player or a team can be studied by finding unique passing motifs from the patterns in the subgraphs of a possession-passing network of soccer games. These flow motifs can be used to analyze individual players and teams based on the diversity and frequency of their involvement in different motifs. Building on the flow motif analyses, we introduce an expected goals model to measure the effectiveness of each style of play. We also make use of a novel way to represent motif data that is easy to understand and can be used to compare players, teams and seasons. Further, we exploit the relationship between play style and the pass probability matrix to support our analysis. Our data set has the last 4 seasons of 6 big European leagues with 8219 matches, 3532 unique players and 155 unique teams. We will use flow motifs to analyze different events, such as for example the transfer of Claudio Bravo to Pep Guardiola's Manchester City, who Jean Seri is and why he must be an elite midfielder and the difference in attacking style between Lionel Messi and Cristiano Ronaldo. Ultimately, an analysis of Post-F{\`{a}}bregas Arsenal is conducted wherein different techniques are combined to analyze the impact the acquisition of Mesut {\"{O}}zil and Alexis S{\'{a}}nchez had on the strategies implemented at Arsenal.},
author = {Bekkers, Joris and Dabadghao, Shaunak},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Flow Motifs in Soccer: What can passing behavior tell us?}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1563.pdf},
year = {2017}
}
@article{Levin2017,
abstract = {Until recently, ranking the skills of golfers on the PGA TOUR was best accomplished by using imprecise summary statistics such as Driving Accuracy and Average Putts Per Round. Since 2003, The PGA TOUR, through their ShotLink Intelligence™ program, has collected detailed shot-level data, which provides coordinates of the locations of shots along with other information. Through the analysis of this data, more fine-grained and precise estimates of the skills of golfers on tour are now possible. The problem of estimating the skill of golfers in different aspects of the game given data from competitions is not simple. This work recognizes a wide array of statistical challenges associated with this problem, which a number of previous approaches to the problem have failed to adequately acknowledge. A brand new approach to the problem is presented which invokes comparisons of the quality of shots taken on the same hole during the same round. The comparisons are utilized in a Network Analysis technique, which is generalized to suit the needs of the problem. This approach is supported with empirical evidence of stronger correlations with the future success of the golfers than the system currently used by the PGA TOUR.},
author = {Levin, Adam},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Ranking the Skills of Golfers on the PGA TOUR using Gradient Boosting Machines and Network Analysis}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1685.pdf},
year = {2017}
}
@misc{Chamberlain2017,
author = {Chamberlain, Alex},
booktitle = {FanGraphs},
title = {{All Aboard the Tyler Saladino Hype Train}},
url = {http://www.fangraphs.com/fantasy/all-aboard-the-tyler-saladino-hype-train/},
urldate = {2017-04-19},
year = {2017}
}
@inproceedings{Alex2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Alex, Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Neural Information Processing Systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Wei2016,
abstract = {The aim of this paper is to discover patterns of player movement and ball striking (short-and long-term shots, and shot combinations) in tennis using HawkEye data which are indicative of changing the probability of winning a point. This is a challenging task because: i) behavior can be unpredictable, ii) the environment is dynamic and the output state-space is large and iii) examples of specific interactions between agents may be limited or non-existent (player A may not have interacted with player B). However, by using a dictionary of discriminative patterns of player behavior, we can form a representation of a player's style, which is interpretable latent factors that allows us to personalize interactions between players based on the match context (opponent, match-score). This approach allows us to perform superior point predictions, and to understand how points are won by systematically creating and exploiting spatiotemporal dominance.},
author = {Wei, Xinyu and Lucey, Patrick and Morgan, Stuart and Reid, Machar and Sridharan, Sridha},
journal = {MIT Sloan Sports Analytics Conference},
title = {{"The Thin Edge of the Wedge": Accurately Predicting Shot Outcomes in Tennis using Style and Context Priors}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2016/02/1475-Other-Sport.pdf},
year = {2016}
}
@misc{Berg2016,
author = {Berg, Ted},
booktitle = {USA Today},
title = {{The 7 most extreme pitches in Major League Baseball}},
url = {http://ftw.usatoday.com/2016/03/mlb-extreme-pitches-gifs-chapman-felix-sale-jansen-familia},
urldate = {2017-04-24},
year = {2016}
}
@article{Oh2015,
abstract = {Conventional approaches to simulate matches have ignored that in basketball the dynamics of ball movement is very sensitive to the lineups on the court and unique identities of players on both offense and defense sides. In this paper, we propose the simulation infrastructure that can bridge the gap between player identity and team level network. We model the progression of a basketball match using a probabilistic graphical model. We model every touch and event in a game as a sequence of transitions between discrete states. We treat the progression of a match as a graph, where each node is a network structure of players on the court, their actions, events, etc., and edges denote possible moves in the game flow. Our results show that either changes in the team lineup or changes in the opponent team lineup significantly affects the dynamics of a match progression. Evaluation on the match data for the 2013-14 NBA season suggests that the graphical model approach is appropriate for modeling a basketball match.},
author = {Oh, Min-Hwan and Keshri, Suraj and Iyengar, Garud},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Graphical Model for Basketball Match Simulation}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2015/02/SSAC15-RP-Finalist-Graphical-model-for-basketball-match-simulation.pdf},
year = {2015}
}
@misc{Keras2015,
author = {Chollet, Fran{\c{c}}ois},
publisher = {GitHub},
title = {{Keras}},
url = {https://github.com/fchollet/keras},
year = {2015}
}
@incollection{RepresentationLearning,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaaron},
booktitle = {Deep Learning},
chapter = {15},
pages = {526--557},
publisher = {MIT Press},
title = {{Representation Learning}},
url = {http://www.deeplearningbook.org/contents/representation.html},
year = {2016}
}
@article{VanderMaaten2008,
abstract = {We present a new technique called " t-SNE " that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {van der Maaten, L J P and Hinton, G E},
journal = {Journal of Machine Learning Research},
keywords = {da mds projection visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
url = {https://lvdmaaten.github.io/publications/papers/JMLR{\_}2008.pdf},
volume = {9},
year = {2008}
}
@article{Johnson2016,
abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-the-art results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
archivePrefix = {arXiv},
arxivId = {1611.04558},
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1611.04558},
journal = {arXiv},
month = {nov},
pages = {1--16},
title = {{Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}},
url = {http://arxiv.org/abs/1611.04558},
year = {2016}
}
@article{Le2017,
abstract = {Current state-of-the-art sports metrics such as “Wins-above-Replacement” in baseball, “Expected Point Value” in basketball, and “Expected Goal Value” in soccer and hockey are now commonplace in performance analysis. These measures have enhanced our ability to compare and value performance in sport.  But they are inherently limited because they are tied to a discrete outcome of a specific event. With the widespread (and growing) availability of player and ball tracking data comes the potential to quantitatively analyze and compare fine-grain movement patterns. An excellent example of this was the “ghosting” system developed by the Toronto Raptors to analyze player decision-making in STATS SportVU tracking data. Specifically, the Raptors created software to predict what a defensive player should have done instead of what they actually did. Motivated by the original “ghosting” work, we showcase an automatic “data-driven ghosting” method using advanced machine learning methodologies called “deep imitation learning”, applied to a season's worth of tracking data from a recent professional league in soccer. Our ghosting method, which avoids substantial manual human annotation, results in a data-driven system that allows us to answer the question “how should this player or team have played in a given game situation compare to the league average?”. In addition, by “fine-tuning” our league average model to the tracking data from a particular team, our ghosting technique can estimate how each team might have approached the situation. Our method enables counterfactual analysis of effectiveness of defensive positioning as both a measurable and viewable quantity for the first time.},
author = {Le, Hoang M. and Carr, Peter and Yue, Yisong and Lucey, Patrick},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Data-Driven Ghosting Using Deep Imitation Learning}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1671-2.pdf},
year = {2017}
}
@article{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X �?Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey E and Pomerleau, Dean and Mitchell, Tom M},
isbn = {9781615679119},
issn = {{\textless}null{\textgreater}},
journal = {Neural Information Processing Systems},
keywords = {Machine learning,zero-shot learning},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
url = {http://papers.nips.cc/paper/3650-zero-shot-learning-with-semantic-output-codes.pdf},
year = {2009}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {cs.CL/1301.3781},
author = {Mikolov, Tomas and Chen, K and Corrado, G and Dean, J},
eprint = {1301.3781},
journal = {ArXiv e-prints},
keywords = {Computer Science - Computation and Language},
primaryClass = {cs.CL},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {https://arxiv.org/pdf/1301.3781.pdf},
year = {2013}
}
@misc{WAR,
booktitle = {FanGraphs},
title = {{What is WAR?}},
url = {http://www.fangraphs.com/library/misc/war/},
urldate = {2017-04-08}
}
@article{Socher2013,
abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. Then, a separate recognition model can be employed for each type. We demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
journal = {Neural Information Processing Systems},
pages = {935--943},
title = {{Zero-shot learning through cross-modal transfer}},
url = {http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf},
year = {2013}
}
@article{Pascanu2013,
abstract = {This paper explores the complexity of deep feedforward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has {\$}kn{\$} hidden units and {\$}n{\_}0{\$} inputs, then the number of linear regions is {\$}O(k{\^{}}{\{}n{\_}0{\}}n{\^{}}{\{}n{\_}0{\}}){\$}. For a {\$}k{\$} layer model with {\$}n{\$} hidden units on each layer it is {\$}\backslashOmega(\backslashleft\backslashlfloor {\{}n{\}}/{\{}n{\_}0{\}}\backslashright\backslashrfloor{\^{}}{\{}k-1{\}}n{\^{}}{\{}n{\_}0{\}}){\$}. The number {\$}\backslashleft\backslashlfloor{\{}n{\}}/{\{}n{\_}0{\}}\backslashright\backslashrfloor{\^{}}{\{}k-1{\}}{\$} grows faster than {\$}k{\^{}}{\{}n{\_}0{\}}{\$} when {\$}n{\$} tends to infinity or when {\$}k{\$} tends to infinity and {\$}n \backslashgeq 2n{\_}0{\$}. Additionally, even when {\$}k{\$} is small, if we restrict {\$}n{\$} to be {\$}2n{\_}0{\$}, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis.},
archivePrefix = {arXiv},
arxivId = {1312.6098},
author = {Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
eprint = {1312.6098},
journal = {Neural Information Processing Systems},
keywords = {artificial neural network,deep learning,hyperplane ar-,rangement,rectifier unit,representational power},
pages = {1--17},
title = {{On the number of response regions of deep feed forward networks with piece-wise linear activations}},
url = {https://arxiv.org/pdf/1312.6098.pdf http://arxiv.org/abs/1312.6098},
year = {2013}
}
@article{Hackett2017,
abstract = {A major analytics challenge in Mixed Martial Arts (MMA) is capturing the differences between fighters that are essential for both establishing matchups and facilitating fan understanding. Here, we model {\~{}}18,000 fighters as mixtures of 10 data-defined prototypical martial arts styles, each with characteristic ways of winning. By balancing fighter-level data with broader trends in MMA, fighter behavior can be predicted even for inexperienced fighters. Beyond providing an informative summary of a fighter's performance, style is a major determinant of success in MMA. This is reflected by the fact that champions of the sport conform to a narrow subset of successful styles.},
author = {Hackett, Sean R. and Storey, John D.},
journal = {MIT Sloan Sports Analytics Conference},
pages = {1--17},
title = {{Mixed Membership Martial Arts: Data-Driven Analysis of Winning Martial Arts Styles}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1575.pdf},
year = {2017}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
month = {aug},
number = {8},
pages = {1798--1828},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
url = {http://ieeexplore.ieee.org/document/6472238/},
volume = {35},
year = {2013}
}
@misc{Retro,
booktitle = {Retrosheet},
title = {{Retrosheet Event Files}},
url = {http://www.retrosheet.org/game.htm},
urldate = {2017-04-08}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
url = {http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
volume = {3},
year = {2003}
}
@article{Schulte2017,
abstract = {Using new game events and location data, we introduce a player performance assessment system that supports drafting, trading, and coaching decisions in the NHL. Players who tend to play in similar locations are clustered together using machine learning techniques, which capture similarity in styles and roles. Clustering players avoids apples-to-oranges comparisons, like comparing offensive and defensive players. Within each cluster, players are ranked according to how much their actions impact their team's chance of scoring the next goal. Our player ranking is based on assigning location-dependent values to actions. A high-resolution Markov model also pinpoints the game situations and rink locations in which players tend to do actions with exceptionally high/low values.},
author = {Schulte, Oliver and Zhao, Zeyu},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Apples-to-Apples: Clustering and Ranking NHL Players Using Location Information and Scoring Impact}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2017/02/1625.pdf},
year = {2017}
}
@misc{Kory2015,
author = {Kory, Matthew},
booktitle = {VICE Sports},
title = {{Paul Goldschmidt Might Really Be This Good}},
url = {https://sports.vice.com/en{\_}us/article/paul-goldschmidt-might-really-be-this-good},
urldate = {2017-04-19},
year = {2015}
}
@article{Wang2016,
abstract = {The amount of raw information available for basketball analytics has been given a great boost with the availability of player tracking data. This facilitates detailed analyses of player movement patterns. In this paper, we focus on the difficult problem of offensive playcall classification. While outstanding individual players are crucial for the success of a team, the strategies that a team can execute and their understanding of the opposing team's strategies also greatly influence game outcomes. These strategies often involve complex interactions between players. We apply techniques from machine learning to directly process SportVU tracking data, specifically variants of neural networks. Our system can label as many sequences with the correct playcall given roughly double the data a human expert needs with high precision, but takes only a fraction of the time. We also show that the system can achieve good recognition rates when trained on one season and tested on the next.},
author = {Wang, Kuan-Chieh and Zemel, Richard},
journal = {MIT Sloan Sports Analytics Conference},
title = {{Classifying NBA Offensive Plays Using Neural Networks}},
url = {http://www.sloansportsconference.com/wp-content/uploads/2016/02/1536-Classifying-NBA-Offensive-Plays-Using-Neural-Networks.pdf},
year = {2016}
}
@misc{Romano2015,
author = {Romano, Ryan},
booktitle = {Beyond the Box Score},
title = {{Carlos Carrasco's incredible, vexing slider}},
url = {http://www.beyondtheboxscore.com/2015/7/22/9007741/carlos-carrascos-incredible-vexing-slider-indians-mlb},
urldate = {2017-04-24},
year = {2015}
}
